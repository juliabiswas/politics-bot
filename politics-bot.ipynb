{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "politicalcommentarybot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliabiswas/politics-bot/blob/master/politics-bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wCPNOiN-Rejv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Importing the Libraries"
      ]
    },
    {
      "metadata": {
        "id": "EAI1AdDC9c_0",
        "colab_type": "code",
        "outputId": "f954eb9d-6c42-41d9-ca2c-bfaa44a1417c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#importing software libraries\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import csv\n",
        "import string\n",
        "import random as rand\n",
        "from pickle import dump, load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "WXxCVE9DRmkX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Loading the Data"
      ]
    },
    {
      "metadata": {
        "id": "7ghsZ9yUKysB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7580acee-2465-4c04-e340-fb124d8868c2"
      },
      "cell_type": "code",
      "source": [
        "#accessing drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-emK8WLHcK-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loading data and creates a list with each comment as an element\n",
        "comments = list()\n",
        "with open('/content/drive/My Drive/...') as csvfile: #add location of file in your drive\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "      #turns each tweet into a string (instead of row in csv)\n",
        "      row = ''.join(row)\n",
        "      #adds each tweet to the sequences list\n",
        "      comments.append(row);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQe4Csn_R0y_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Cleaning the Data"
      ]
    },
    {
      "metadata": {
        "id": "_LeT9zIixZ3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cleaning each comment\n",
        "def clean(comment):\n",
        "  comment = comment.replace('--', ' ')\n",
        "  tokens = comment.split()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        " \n",
        "#organizing into sequences\n",
        "sequences = list()\n",
        "length = 15 #14 tokens will be the input and 1 token will be the output\n",
        "for comment in comments: #each comment is split into sequences\n",
        "  tokens = clean(comment)\n",
        "  for i in range(length, len(tokens)):\n",
        "\t   seq = tokens[i-length:i]\n",
        "\t   line = ' '.join(seq)\n",
        "\t   sequences.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FiKTqWnLR28G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "03ae4hUo9h1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7296
        },
        "outputId": "06db34bc-12ac-4379-d4b4-77fb8d34f0b6"
      },
      "cell_type": "code",
      "source": [
        "#encoding sequences as integers\n",
        "stored_sequences = sequences #saves a copy of sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "sequences = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "#calculating vocab size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        " \n",
        "#separating into input and output\n",
        "sequences = array(sequences)\n",
        "x, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = x.shape[1]\n",
        " \n",
        "#defining the model\n",
        "mod = Sequential()\n",
        "mod.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "mod.add(LSTM(100, return_sequences=True))\n",
        "mod.add(LSTM(100, return_sequences=True))\n",
        "mod.add(LSTM(100))\n",
        "mod.add(Dense(100, activation='relu'))\n",
        "mod.add(Dense(vocab_size, activation='softmax'))\n",
        "print(mod.summary())\n",
        "\n",
        "#compiling the model\n",
        "mod.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#fitting the model\n",
        "mod.fit(x, y, batch_size=128, epochs=200)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 14, 50)            245250    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 14, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 14, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4905)              495405    \n",
            "=================================================================\n",
            "Total params: 971,955\n",
            "Trainable params: 971,955\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/200\n",
            "21031/21031 [==============================] - 34s 2ms/step - loss: 7.1310 - acc: 0.0555\n",
            "Epoch 2/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.6726 - acc: 0.0578\n",
            "Epoch 3/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.6511 - acc: 0.0578\n",
            "Epoch 4/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.6413 - acc: 0.0578\n",
            "Epoch 5/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.5584 - acc: 0.0578\n",
            "Epoch 6/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.4038 - acc: 0.0580\n",
            "Epoch 7/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 6.2953 - acc: 0.0627\n",
            "Epoch 8/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.2231 - acc: 0.0705\n",
            "Epoch 9/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.1524 - acc: 0.0737\n",
            "Epoch 10/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 6.0797 - acc: 0.0758\n",
            "Epoch 11/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.9973 - acc: 0.0779\n",
            "Epoch 12/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.9121 - acc: 0.0824\n",
            "Epoch 13/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 5.8399 - acc: 0.0845\n",
            "Epoch 14/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.7741 - acc: 0.0866\n",
            "Epoch 15/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.7128 - acc: 0.0862\n",
            "Epoch 16/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.6578 - acc: 0.0873\n",
            "Epoch 17/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 5.6015 - acc: 0.0920\n",
            "Epoch 18/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.5515 - acc: 0.0915\n",
            "Epoch 19/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.5096 - acc: 0.0928\n",
            "Epoch 20/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.4586 - acc: 0.0930\n",
            "Epoch 21/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.4109 - acc: 0.0966\n",
            "Epoch 22/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.3607 - acc: 0.0985\n",
            "Epoch 23/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.3100 - acc: 0.1014\n",
            "Epoch 24/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.2650 - acc: 0.1050\n",
            "Epoch 25/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.2135 - acc: 0.1084\n",
            "Epoch 26/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.1631 - acc: 0.1123\n",
            "Epoch 27/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.1173 - acc: 0.1160\n",
            "Epoch 28/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 5.0579 - acc: 0.1225\n",
            "Epoch 29/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.9964 - acc: 0.1281\n",
            "Epoch 30/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.9484 - acc: 0.1339\n",
            "Epoch 31/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.8920 - acc: 0.1367\n",
            "Epoch 32/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.8398 - acc: 0.1401\n",
            "Epoch 33/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.7845 - acc: 0.1436\n",
            "Epoch 34/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.7244 - acc: 0.1497\n",
            "Epoch 35/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.6841 - acc: 0.1545\n",
            "Epoch 36/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.6282 - acc: 0.1584\n",
            "Epoch 37/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 4.5629 - acc: 0.1652\n",
            "Epoch 38/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.5163 - acc: 0.1686\n",
            "Epoch 39/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 4.4632 - acc: 0.1736\n",
            "Epoch 40/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 4.4040 - acc: 0.1775\n",
            "Epoch 41/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 4.3633 - acc: 0.1785\n",
            "Epoch 42/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.3154 - acc: 0.1837\n",
            "Epoch 43/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.2672 - acc: 0.1881\n",
            "Epoch 44/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 4.2119 - acc: 0.1928\n",
            "Epoch 45/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.1739 - acc: 0.1945\n",
            "Epoch 46/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.1260 - acc: 0.2006\n",
            "Epoch 47/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.0875 - acc: 0.2036\n",
            "Epoch 48/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 4.0336 - acc: 0.2055\n",
            "Epoch 49/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.9824 - acc: 0.2102\n",
            "Epoch 50/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.9409 - acc: 0.2137\n",
            "Epoch 51/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.9113 - acc: 0.2173\n",
            "Epoch 52/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.8616 - acc: 0.2199\n",
            "Epoch 53/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.8139 - acc: 0.2276\n",
            "Epoch 54/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.7779 - acc: 0.2300\n",
            "Epoch 55/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.7376 - acc: 0.2301\n",
            "Epoch 56/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 3.7023 - acc: 0.2351\n",
            "Epoch 57/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 3.6687 - acc: 0.2362\n",
            "Epoch 58/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.6240 - acc: 0.2405\n",
            "Epoch 59/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.5832 - acc: 0.2472\n",
            "Epoch 60/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.5520 - acc: 0.2491\n",
            "Epoch 61/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.5142 - acc: 0.2528\n",
            "Epoch 62/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.4893 - acc: 0.2565\n",
            "Epoch 63/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.4480 - acc: 0.2585\n",
            "Epoch 64/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.4078 - acc: 0.2631\n",
            "Epoch 65/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.3762 - acc: 0.2681\n",
            "Epoch 66/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.3505 - acc: 0.2706\n",
            "Epoch 67/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 3.3010 - acc: 0.2767\n",
            "Epoch 68/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.2623 - acc: 0.2852\n",
            "Epoch 69/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.2526 - acc: 0.2823\n",
            "Epoch 70/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.2258 - acc: 0.2841\n",
            "Epoch 71/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.1922 - acc: 0.2896\n",
            "Epoch 72/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 3.1601 - acc: 0.2927\n",
            "Epoch 73/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.1210 - acc: 0.2995\n",
            "Epoch 74/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.0913 - acc: 0.3052\n",
            "Epoch 75/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.0687 - acc: 0.3078\n",
            "Epoch 76/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 3.0298 - acc: 0.3146\n",
            "Epoch 77/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 3.0023 - acc: 0.3179\n",
            "Epoch 78/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.9751 - acc: 0.3252\n",
            "Epoch 79/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.9515 - acc: 0.3261\n",
            "Epoch 80/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.9275 - acc: 0.3290\n",
            "Epoch 81/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.8952 - acc: 0.3338\n",
            "Epoch 82/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.8845 - acc: 0.3328\n",
            "Epoch 83/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.8554 - acc: 0.3383\n",
            "Epoch 84/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.8298 - acc: 0.3436\n",
            "Epoch 85/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.8152 - acc: 0.3424\n",
            "Epoch 86/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.7783 - acc: 0.3501\n",
            "Epoch 87/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.7570 - acc: 0.3546\n",
            "Epoch 88/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.7379 - acc: 0.3575\n",
            "Epoch 89/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.7323 - acc: 0.3575\n",
            "Epoch 90/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.6982 - acc: 0.3620\n",
            "Epoch 91/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.6779 - acc: 0.3657\n",
            "Epoch 92/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.6401 - acc: 0.3711\n",
            "Epoch 93/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.6080 - acc: 0.3763\n",
            "Epoch 94/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.6176 - acc: 0.3780\n",
            "Epoch 95/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.5790 - acc: 0.3822\n",
            "Epoch 96/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.5424 - acc: 0.3916\n",
            "Epoch 97/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.5226 - acc: 0.3974\n",
            "Epoch 98/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.5259 - acc: 0.3908\n",
            "Epoch 99/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.4998 - acc: 0.3961\n",
            "Epoch 100/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.4615 - acc: 0.4055\n",
            "Epoch 101/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.4666 - acc: 0.4017\n",
            "Epoch 102/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.4449 - acc: 0.4051\n",
            "Epoch 103/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.4096 - acc: 0.4159\n",
            "Epoch 104/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.3909 - acc: 0.4191\n",
            "Epoch 105/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.4057 - acc: 0.4148\n",
            "Epoch 106/200\n",
            "21031/21031 [==============================] - 32s 1ms/step - loss: 2.3843 - acc: 0.4185\n",
            "Epoch 107/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.3363 - acc: 0.4288\n",
            "Epoch 108/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.3251 - acc: 0.4322\n",
            "Epoch 109/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.3142 - acc: 0.4339\n",
            "Epoch 110/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.2863 - acc: 0.4405\n",
            "Epoch 111/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.2519 - acc: 0.4438\n",
            "Epoch 112/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.2285 - acc: 0.4503\n",
            "Epoch 113/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.2168 - acc: 0.4538\n",
            "Epoch 114/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.2137 - acc: 0.4555\n",
            "Epoch 115/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.2036 - acc: 0.4536\n",
            "Epoch 116/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.1701 - acc: 0.4598\n",
            "Epoch 117/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.1427 - acc: 0.4653\n",
            "Epoch 118/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.1417 - acc: 0.4675\n",
            "Epoch 119/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.1208 - acc: 0.4726\n",
            "Epoch 120/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.1217 - acc: 0.4687\n",
            "Epoch 121/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.0953 - acc: 0.4758\n",
            "Epoch 122/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.0809 - acc: 0.4785\n",
            "Epoch 123/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 2.0708 - acc: 0.4800\n",
            "Epoch 124/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.0496 - acc: 0.4840\n",
            "Epoch 125/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.0429 - acc: 0.4881\n",
            "Epoch 126/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 2.0398 - acc: 0.4857\n",
            "Epoch 127/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 2.0213 - acc: 0.4890\n",
            "Epoch 128/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.9747 - acc: 0.5009\n",
            "Epoch 129/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.9559 - acc: 0.5067\n",
            "Epoch 130/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.9419 - acc: 0.5099\n",
            "Epoch 131/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.9129 - acc: 0.5181\n",
            "Epoch 132/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.9082 - acc: 0.5206\n",
            "Epoch 133/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.9045 - acc: 0.5200\n",
            "Epoch 134/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.9125 - acc: 0.5141\n",
            "Epoch 135/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.9017 - acc: 0.5179\n",
            "Epoch 136/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.8883 - acc: 0.5172\n",
            "Epoch 137/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 1.8821 - acc: 0.5213\n",
            "Epoch 138/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.8512 - acc: 0.5293\n",
            "Epoch 139/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.8141 - acc: 0.5375\n",
            "Epoch 140/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.7820 - acc: 0.5471\n",
            "Epoch 141/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.7758 - acc: 0.5478\n",
            "Epoch 142/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.7742 - acc: 0.5475\n",
            "Epoch 143/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.7760 - acc: 0.5474\n",
            "Epoch 144/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.7586 - acc: 0.5504\n",
            "Epoch 145/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.7356 - acc: 0.5551\n",
            "Epoch 146/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.7331 - acc: 0.5573\n",
            "Epoch 147/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.7003 - acc: 0.5657\n",
            "Epoch 148/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.6739 - acc: 0.5728\n",
            "Epoch 149/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.6687 - acc: 0.5670\n",
            "Epoch 150/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.6769 - acc: 0.5660\n",
            "Epoch 151/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.6674 - acc: 0.5698\n",
            "Epoch 152/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.6756 - acc: 0.5682\n",
            "Epoch 153/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.6717 - acc: 0.5685\n",
            "Epoch 154/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.6276 - acc: 0.5815\n",
            "Epoch 155/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5926 - acc: 0.5908\n",
            "Epoch 156/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5978 - acc: 0.5892\n",
            "Epoch 157/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5694 - acc: 0.5963\n",
            "Epoch 158/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5682 - acc: 0.5918\n",
            "Epoch 159/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5444 - acc: 0.6043\n",
            "Epoch 160/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5367 - acc: 0.6020\n",
            "Epoch 161/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5334 - acc: 0.6015\n",
            "Epoch 162/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5413 - acc: 0.5982\n",
            "Epoch 163/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.5462 - acc: 0.6010\n",
            "Epoch 164/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5326 - acc: 0.6008\n",
            "Epoch 165/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5085 - acc: 0.6062\n",
            "Epoch 166/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.5085 - acc: 0.6073\n",
            "Epoch 167/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.4494 - acc: 0.6274\n",
            "Epoch 168/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.4446 - acc: 0.6290\n",
            "Epoch 169/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.4492 - acc: 0.6261\n",
            "Epoch 170/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.4147 - acc: 0.6337\n",
            "Epoch 171/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.4407 - acc: 0.6256\n",
            "Epoch 172/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.4520 - acc: 0.6216\n",
            "Epoch 173/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.4487 - acc: 0.6204\n",
            "Epoch 174/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3926 - acc: 0.6353\n",
            "Epoch 175/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3726 - acc: 0.6436\n",
            "Epoch 176/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.3625 - acc: 0.6474\n",
            "Epoch 177/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.3674 - acc: 0.6435\n",
            "Epoch 178/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3577 - acc: 0.6469\n",
            "Epoch 179/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.3270 - acc: 0.6553\n",
            "Epoch 180/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3171 - acc: 0.6588\n",
            "Epoch 181/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3341 - acc: 0.6535\n",
            "Epoch 182/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3499 - acc: 0.6497\n",
            "Epoch 183/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3355 - acc: 0.6497\n",
            "Epoch 184/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.3019 - acc: 0.6600\n",
            "Epoch 185/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2860 - acc: 0.6628\n",
            "Epoch 186/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2578 - acc: 0.6730\n",
            "Epoch 187/200\n",
            "21031/21031 [==============================] - 30s 1ms/step - loss: 1.2613 - acc: 0.6702\n",
            "Epoch 188/200\n",
            "21031/21031 [==============================] - 32s 1ms/step - loss: 1.2389 - acc: 0.6749\n",
            "Epoch 189/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2402 - acc: 0.6752\n",
            "Epoch 190/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2196 - acc: 0.6815\n",
            "Epoch 191/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2408 - acc: 0.6747\n",
            "Epoch 192/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2573 - acc: 0.6677\n",
            "Epoch 193/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2834 - acc: 0.6591\n",
            "Epoch 194/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.2334 - acc: 0.6745\n",
            "Epoch 195/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.1909 - acc: 0.6894\n",
            "Epoch 196/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.1524 - acc: 0.6969\n",
            "Epoch 197/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.1426 - acc: 0.7009\n",
            "Epoch 198/200\n",
            "21031/21031 [==============================] - 32s 2ms/step - loss: 1.1393 - acc: 0.7016\n",
            "Epoch 199/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.1487 - acc: 0.7018\n",
            "Epoch 200/200\n",
            "21031/21031 [==============================] - 31s 1ms/step - loss: 1.1723 - acc: 0.6896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc4fa86e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "D18NNTPZSvpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Using the Model"
      ]
    },
    {
      "metadata": {
        "id": "Xn4yVh1EDB3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a144278f-b2fe-4259-c2fd-c1e1aa80a538"
      },
      "cell_type": "code",
      "source": [
        "#generating a comment using the model\n",
        "def generate(mod, tokenizer, seq_length, seed_comment, n_words):\n",
        "\tresult = list()\n",
        "\tin_comment = seed_comment\n",
        "\tfor i in range(0, n_words):\n",
        "\t\t#text to integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_comment])[0]\n",
        "\t\t#truncating to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t#predicting probabilities\n",
        "\t\tprob = mod.predict_classes(encoded, verbose=0)\n",
        "\t\t#word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == prob:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t#adding to input\n",
        "\t\tin_comment += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "#calculating the sequence length\n",
        "seq_length = len(stored_sequences[0].split()) - 1\n",
        "\n",
        "#selecting a seed comment\n",
        "seed_comment = stored_sequences[rand.randint(0,len(stored_sequences))]\n",
        " \n",
        "#generating a new comment\n",
        "generated = generate(mod, tokenizer, seq_length, seed_comment, 14)\n",
        "print(generated)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "motive in withdrawing from the paris accord like his insistance on americas no president\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}