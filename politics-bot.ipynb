{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "politicalcommentarybot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliabiswas/politics-bot/blob/master/politics-bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCPNOiN-Rejv"
      },
      "source": [
        "##Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAI1AdDC9c_0"
      },
      "source": [
        "#importing software libraries\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import csv\n",
        "import string\n",
        "import random as rand\n",
        "from pickle import dump, load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXxCVE9DRmkX"
      },
      "source": [
        "##Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ghsZ9yUKysB"
      },
      "source": [
        "#accessing drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emK8WLHcK-9"
      },
      "source": [
        "#loading data and creates a list with each comment as an element\n",
        "comments = []\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/juliabiswas/politics-bot/master/comments.csv')\n",
        "data.columns = ['comment']\n",
        "comments = data['comment']"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQe4Csn_R0y_"
      },
      "source": [
        "##Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeT9zIixZ3z"
      },
      "source": [
        "#cleaning each comment\n",
        "def clean(comment):\n",
        "  comment = comment.replace('--', ' ')\n",
        "  tokens = comment.split()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        " \n",
        "#organizing into sequences\n",
        "sequences = []\n",
        "length = 15 #14 tokens will be the input and 1 token will be the output\n",
        "for comment in comments: #each comment is split into sequences\n",
        "  tokens = clean(comment)\n",
        "  for i in range(length, len(tokens)):\n",
        "    seq = tokens[i-length:i]\n",
        "    line = ' '.join(seq)\n",
        "    sequences.append(line)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiKTqWnLR28G"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ae4hUo9h1Y",
        "outputId": "a953f3d0-7142-41b5-8b93-ff28e83c1029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "#encoding sequences as integers\n",
        "stored_sequences = sequences #saves a copy of sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "sequences = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "#calculating vocab size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        " \n",
        "#separating into input and output\n",
        "sequences = array(sequences)\n",
        "x, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = x.shape[1]\n",
        " \n",
        "#defining the model\n",
        "mod = Sequential()\n",
        "mod.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "mod.add(LSTM(100, return_sequences=True))\n",
        "mod.add(LSTM(100, return_sequences=True))\n",
        "mod.add(LSTM(100))\n",
        "mod.add(Dense(100, activation='relu'))\n",
        "mod.add(Dense(vocab_size, activation='softmax'))\n",
        "print(mod.summary())\n",
        "\n",
        "#compiling the model\n",
        "mod.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#fitting the model\n",
        "mod.fit(x, y, batch_size=128, epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 14, 50)            244400    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 14, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 14, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4888)              493688    \n",
            "=================================================================\n",
            "Total params: 969,388\n",
            "Trainable params: 969,388\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            " 72/164 [============>.................] - ETA: 11s - loss: 7.3587 - accuracy: 0.0492"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18NNTPZSvpS"
      },
      "source": [
        "##Using the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4yVh1EDB3W"
      },
      "source": [
        "#generating a comment using the model\n",
        "def generate(mod, tokenizer, seq_length, seed_comment, n_words):\n",
        "\tresult = list()\n",
        "\tin_comment = seed_comment\n",
        "\tfor i in range(0, n_words):\n",
        "\t\t#text to integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_comment])[0]\n",
        "\t\t#truncating to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t#predicting probabilities\n",
        "\t\tprob = mod.predict_classes(encoded, verbose=0)\n",
        "\t\t#word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == prob:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t#adding to input\n",
        "\t\tin_comment += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "#calculating the sequence length\n",
        "seq_length = len(stored_sequences[0].split()) - 1\n",
        "\n",
        "#selecting a seed comment\n",
        "seed_comment = stored_sequences[rand.randint(0,len(stored_sequences))]\n",
        " \n",
        "#generating a new comment\n",
        "generated = generate(mod, tokenizer, seq_length, seed_comment, 14)\n",
        "print(generated)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}