{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "politicalcommentarybot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliabiswas/politics-bot/blob/master/politics-bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCPNOiN-Rejv"
      },
      "source": [
        "##Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAI1AdDC9c_0"
      },
      "source": [
        "#importing software libraries\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import string\n",
        "import random as rand\n",
        "from pickle import dump, load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXxCVE9DRmkX"
      },
      "source": [
        "##Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emK8WLHcK-9"
      },
      "source": [
        "#loading data and creates a list with each comment as an element\n",
        "comments = []\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/juliabiswas/politics-bot/master/comments.csv')\n",
        "data.columns = ['comment']\n",
        "comments = data['comment']"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQe4Csn_R0y_"
      },
      "source": [
        "##Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LeT9zIixZ3z"
      },
      "source": [
        "#cleaning each comment\n",
        "def clean(comment):\n",
        "  comment = comment.replace('--', ' ')\n",
        "  tokens = comment.split()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens\n",
        " \n",
        "#organizing into sequences\n",
        "sequences = []\n",
        "length = 15 #14 tokens will be the input and 1 token will be the output\n",
        "for comment in comments: #each comment is split into sequences\n",
        "  tokens = clean(comment)\n",
        "  for i in range(length, len(tokens)):\n",
        "    seq = tokens[i-length:i]\n",
        "    line = ' '.join(seq)\n",
        "    sequences.append(line)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiKTqWnLR28G"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ae4hUo9h1Y",
        "outputId": "5bc6627c-db55-4075-f62f-23ece6612930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#encoding sequences as integers\n",
        "stored_sequences = sequences #saves a copy of sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "sequences = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "#calculating vocab size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        " \n",
        "#separating into input and output\n",
        "sequences = array(sequences)\n",
        "x, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = x.shape[1]\n",
        " \n",
        "#defining the model\n",
        "mod = Sequential()\n",
        "mod.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "mod.add(LSTM(100, return_sequences=True))\n",
        "mod.add(LSTM(100, return_sequences=True))\n",
        "mod.add(LSTM(100))\n",
        "mod.add(Dense(100, activation='relu'))\n",
        "mod.add(Dense(vocab_size, activation='softmax'))\n",
        "print(mod.summary())\n",
        "\n",
        "#compiling the model\n",
        "mod.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#fitting the model\n",
        "mod.fit(x, y, batch_size=128, epochs=200)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 14, 50)            244400    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 14, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 14, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4888)              493688    \n",
            "=================================================================\n",
            "Total params: 969,388\n",
            "Trainable params: 969,388\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 7.1166 - accuracy: 0.0529\n",
            "Epoch 2/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 6.6721 - accuracy: 0.0579\n",
            "Epoch 3/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 6.6501 - accuracy: 0.0579\n",
            "Epoch 4/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 6.6437 - accuracy: 0.0579\n",
            "Epoch 5/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 6.6389 - accuracy: 0.0579\n",
            "Epoch 6/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 6.6142 - accuracy: 0.0579\n",
            "Epoch 7/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 6.4928 - accuracy: 0.0584\n",
            "Epoch 8/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 6.3861 - accuracy: 0.0642\n",
            "Epoch 9/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 6.2682 - accuracy: 0.0725\n",
            "Epoch 10/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 6.1637 - accuracy: 0.0771\n",
            "Epoch 11/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 6.0727 - accuracy: 0.0828\n",
            "Epoch 12/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 5.9858 - accuracy: 0.0876\n",
            "Epoch 13/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 5.9029 - accuracy: 0.0892\n",
            "Epoch 14/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 5.8240 - accuracy: 0.0930\n",
            "Epoch 15/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 5.7514 - accuracy: 0.0967\n",
            "Epoch 16/200\n",
            "164/164 [==============================] - 20s 123ms/step - loss: 5.6859 - accuracy: 0.1003\n",
            "Epoch 17/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 5.6235 - accuracy: 0.1026\n",
            "Epoch 18/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 5.5644 - accuracy: 0.1067\n",
            "Epoch 19/200\n",
            "164/164 [==============================] - 20s 123ms/step - loss: 5.5016 - accuracy: 0.1078\n",
            "Epoch 20/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 5.4348 - accuracy: 0.1128\n",
            "Epoch 21/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 5.3737 - accuracy: 0.1182\n",
            "Epoch 22/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 5.3122 - accuracy: 0.1212\n",
            "Epoch 23/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 5.2596 - accuracy: 0.1233\n",
            "Epoch 24/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 5.2016 - accuracy: 0.1285\n",
            "Epoch 25/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 5.1458 - accuracy: 0.1333\n",
            "Epoch 26/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 5.0863 - accuracy: 0.1347\n",
            "Epoch 27/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 5.0436 - accuracy: 0.1391\n",
            "Epoch 28/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 4.9896 - accuracy: 0.1404\n",
            "Epoch 29/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 4.9435 - accuracy: 0.1442\n",
            "Epoch 30/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.8906 - accuracy: 0.1463\n",
            "Epoch 31/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.8411 - accuracy: 0.1482\n",
            "Epoch 32/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 4.7933 - accuracy: 0.1516\n",
            "Epoch 33/200\n",
            "164/164 [==============================] - 21s 130ms/step - loss: 4.7504 - accuracy: 0.1533\n",
            "Epoch 34/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 4.7066 - accuracy: 0.1571\n",
            "Epoch 35/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.6584 - accuracy: 0.1585\n",
            "Epoch 36/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.6152 - accuracy: 0.1614\n",
            "Epoch 37/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.5695 - accuracy: 0.1636\n",
            "Epoch 38/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 4.5272 - accuracy: 0.1671\n",
            "Epoch 39/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.4866 - accuracy: 0.1707\n",
            "Epoch 40/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 4.4479 - accuracy: 0.1708\n",
            "Epoch 41/200\n",
            "164/164 [==============================] - 20s 119ms/step - loss: 4.4050 - accuracy: 0.1737\n",
            "Epoch 42/200\n",
            "164/164 [==============================] - 20s 119ms/step - loss: 4.3650 - accuracy: 0.1773\n",
            "Epoch 43/200\n",
            "164/164 [==============================] - 20s 120ms/step - loss: 4.3247 - accuracy: 0.1783\n",
            "Epoch 44/200\n",
            "164/164 [==============================] - 20s 120ms/step - loss: 4.2766 - accuracy: 0.1837\n",
            "Epoch 45/200\n",
            "164/164 [==============================] - 22s 133ms/step - loss: 4.2453 - accuracy: 0.1853\n",
            "Epoch 46/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 4.2158 - accuracy: 0.1877\n",
            "Epoch 47/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 4.1722 - accuracy: 0.1895\n",
            "Epoch 48/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 4.1331 - accuracy: 0.1939\n",
            "Epoch 49/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 4.1074 - accuracy: 0.1945\n",
            "Epoch 50/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 4.0636 - accuracy: 0.1980\n",
            "Epoch 51/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 4.0261 - accuracy: 0.1991\n",
            "Epoch 52/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 3.9961 - accuracy: 0.2030\n",
            "Epoch 53/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 3.9565 - accuracy: 0.2048\n",
            "Epoch 54/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.9292 - accuracy: 0.2058\n",
            "Epoch 55/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.8950 - accuracy: 0.2104\n",
            "Epoch 56/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.8527 - accuracy: 0.2162\n",
            "Epoch 57/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.8249 - accuracy: 0.2164\n",
            "Epoch 58/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.8000 - accuracy: 0.2179\n",
            "Epoch 59/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.7563 - accuracy: 0.2208\n",
            "Epoch 60/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.7248 - accuracy: 0.2263\n",
            "Epoch 61/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.6975 - accuracy: 0.2287\n",
            "Epoch 62/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.6604 - accuracy: 0.2335\n",
            "Epoch 63/200\n",
            "164/164 [==============================] - 22s 132ms/step - loss: 3.6416 - accuracy: 0.2332\n",
            "Epoch 64/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.6076 - accuracy: 0.2384\n",
            "Epoch 65/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.5726 - accuracy: 0.2395\n",
            "Epoch 66/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.5359 - accuracy: 0.2467\n",
            "Epoch 67/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 3.5167 - accuracy: 0.2491\n",
            "Epoch 68/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 3.4906 - accuracy: 0.2496\n",
            "Epoch 69/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.4739 - accuracy: 0.2526\n",
            "Epoch 70/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.4272 - accuracy: 0.2579\n",
            "Epoch 71/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.3816 - accuracy: 0.2617\n",
            "Epoch 72/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.3566 - accuracy: 0.2657\n",
            "Epoch 73/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.3294 - accuracy: 0.2697\n",
            "Epoch 74/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.3012 - accuracy: 0.2739\n",
            "Epoch 75/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.2719 - accuracy: 0.2759\n",
            "Epoch 76/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 3.2285 - accuracy: 0.2848\n",
            "Epoch 77/200\n",
            "164/164 [==============================] - 21s 130ms/step - loss: 3.1929 - accuracy: 0.2865\n",
            "Epoch 78/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.1615 - accuracy: 0.2904\n",
            "Epoch 79/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 3.1243 - accuracy: 0.2969\n",
            "Epoch 80/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 3.0968 - accuracy: 0.3001\n",
            "Epoch 81/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 3.0693 - accuracy: 0.3057\n",
            "Epoch 82/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 3.0170 - accuracy: 0.3148\n",
            "Epoch 83/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 2.9865 - accuracy: 0.3202\n",
            "Epoch 84/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.9580 - accuracy: 0.3215\n",
            "Epoch 85/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 2.9318 - accuracy: 0.3258\n",
            "Epoch 86/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 2.9062 - accuracy: 0.3318\n",
            "Epoch 87/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 2.8766 - accuracy: 0.3361\n",
            "Epoch 88/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.8422 - accuracy: 0.3426\n",
            "Epoch 89/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.8094 - accuracy: 0.3503\n",
            "Epoch 90/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 2.7996 - accuracy: 0.3478\n",
            "Epoch 91/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.7705 - accuracy: 0.3529\n",
            "Epoch 92/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 2.7337 - accuracy: 0.3611\n",
            "Epoch 93/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.6858 - accuracy: 0.3678\n",
            "Epoch 94/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.6521 - accuracy: 0.3759\n",
            "Epoch 95/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.6318 - accuracy: 0.3751\n",
            "Epoch 96/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.6010 - accuracy: 0.3828\n",
            "Epoch 97/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 2.5936 - accuracy: 0.3866\n",
            "Epoch 98/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.5536 - accuracy: 0.3927\n",
            "Epoch 99/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.5178 - accuracy: 0.4008\n",
            "Epoch 100/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 2.4938 - accuracy: 0.4040\n",
            "Epoch 101/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.4725 - accuracy: 0.4063\n",
            "Epoch 102/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.4507 - accuracy: 0.4096\n",
            "Epoch 103/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.4197 - accuracy: 0.4223\n",
            "Epoch 104/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.3947 - accuracy: 0.4245\n",
            "Epoch 105/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.3743 - accuracy: 0.4300\n",
            "Epoch 106/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.3431 - accuracy: 0.4339\n",
            "Epoch 107/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 2.3194 - accuracy: 0.4407\n",
            "Epoch 108/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 2.2874 - accuracy: 0.4438\n",
            "Epoch 109/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 2.2669 - accuracy: 0.4508\n",
            "Epoch 110/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 2.2475 - accuracy: 0.4510\n",
            "Epoch 111/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.2118 - accuracy: 0.4607\n",
            "Epoch 112/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 2.1673 - accuracy: 0.4710\n",
            "Epoch 113/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 2.1487 - accuracy: 0.4751\n",
            "Epoch 114/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 2.1247 - accuracy: 0.4797\n",
            "Epoch 115/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.1192 - accuracy: 0.4811\n",
            "Epoch 116/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.0934 - accuracy: 0.4853\n",
            "Epoch 117/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 2.0878 - accuracy: 0.4846\n",
            "Epoch 118/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 2.0814 - accuracy: 0.4858\n",
            "Epoch 119/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 2.0427 - accuracy: 0.4940\n",
            "Epoch 120/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.9993 - accuracy: 0.5054\n",
            "Epoch 121/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.9659 - accuracy: 0.5144\n",
            "Epoch 122/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 1.9376 - accuracy: 0.5233\n",
            "Epoch 123/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.9565 - accuracy: 0.5147\n",
            "Epoch 124/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.9380 - accuracy: 0.5159\n",
            "Epoch 125/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.8977 - accuracy: 0.5265\n",
            "Epoch 126/200\n",
            "164/164 [==============================] - 21s 130ms/step - loss: 1.8554 - accuracy: 0.5395\n",
            "Epoch 127/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.8300 - accuracy: 0.5420\n",
            "Epoch 128/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.8188 - accuracy: 0.5425\n",
            "Epoch 129/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.7999 - accuracy: 0.5512\n",
            "Epoch 130/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.7981 - accuracy: 0.5477\n",
            "Epoch 131/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.7810 - accuracy: 0.5507\n",
            "Epoch 132/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.7395 - accuracy: 0.5613\n",
            "Epoch 133/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.6980 - accuracy: 0.5707\n",
            "Epoch 134/200\n",
            "164/164 [==============================] - 20s 123ms/step - loss: 1.6684 - accuracy: 0.5813\n",
            "Epoch 135/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 1.6653 - accuracy: 0.5815\n",
            "Epoch 136/200\n",
            "164/164 [==============================] - 20s 122ms/step - loss: 1.6393 - accuracy: 0.5863\n",
            "Epoch 137/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 1.6220 - accuracy: 0.5901\n",
            "Epoch 138/200\n",
            "164/164 [==============================] - 20s 123ms/step - loss: 1.6113 - accuracy: 0.5942\n",
            "Epoch 139/200\n",
            "164/164 [==============================] - 20s 123ms/step - loss: 1.5893 - accuracy: 0.6003\n",
            "Epoch 140/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.5657 - accuracy: 0.6021\n",
            "Epoch 141/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 1.5511 - accuracy: 0.6046\n",
            "Epoch 142/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.5330 - accuracy: 0.6128\n",
            "Epoch 143/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.5226 - accuracy: 0.6135\n",
            "Epoch 144/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.4864 - accuracy: 0.6232\n",
            "Epoch 145/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.4765 - accuracy: 0.6214\n",
            "Epoch 146/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.4564 - accuracy: 0.6274\n",
            "Epoch 147/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.4285 - accuracy: 0.6335\n",
            "Epoch 148/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.4391 - accuracy: 0.6296\n",
            "Epoch 149/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.4134 - accuracy: 0.6379\n",
            "Epoch 150/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.4009 - accuracy: 0.6390\n",
            "Epoch 151/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.3570 - accuracy: 0.6535\n",
            "Epoch 152/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 1.3577 - accuracy: 0.6506\n",
            "Epoch 153/200\n",
            "164/164 [==============================] - 25s 150ms/step - loss: 1.3345 - accuracy: 0.6590\n",
            "Epoch 154/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 1.3004 - accuracy: 0.6685\n",
            "Epoch 155/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.2877 - accuracy: 0.6729\n",
            "Epoch 156/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.3165 - accuracy: 0.6596\n",
            "Epoch 157/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 1.3172 - accuracy: 0.6613\n",
            "Epoch 158/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.2763 - accuracy: 0.6722\n",
            "Epoch 159/200\n",
            "164/164 [==============================] - 20s 123ms/step - loss: 1.2464 - accuracy: 0.6756\n",
            "Epoch 160/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 1.2336 - accuracy: 0.6808\n",
            "Epoch 161/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.2275 - accuracy: 0.6837\n",
            "Epoch 162/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 1.1897 - accuracy: 0.6909\n",
            "Epoch 163/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 1.1714 - accuracy: 0.6990\n",
            "Epoch 164/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.1430 - accuracy: 0.7071\n",
            "Epoch 165/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.1318 - accuracy: 0.7087\n",
            "Epoch 166/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.1378 - accuracy: 0.7041\n",
            "Epoch 167/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 1.1189 - accuracy: 0.7114\n",
            "Epoch 168/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.1054 - accuracy: 0.7146\n",
            "Epoch 169/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.0902 - accuracy: 0.7208\n",
            "Epoch 170/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.0646 - accuracy: 0.7253\n",
            "Epoch 171/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.0307 - accuracy: 0.7344\n",
            "Epoch 172/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.0474 - accuracy: 0.7304\n",
            "Epoch 173/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.0569 - accuracy: 0.7271\n",
            "Epoch 174/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 1.0631 - accuracy: 0.7231\n",
            "Epoch 175/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 1.0326 - accuracy: 0.7346\n",
            "Epoch 176/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 0.9851 - accuracy: 0.7473\n",
            "Epoch 177/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 0.9651 - accuracy: 0.7540\n",
            "Epoch 178/200\n",
            "164/164 [==============================] - 21s 130ms/step - loss: 0.9542 - accuracy: 0.7552\n",
            "Epoch 179/200\n",
            "164/164 [==============================] - 21s 129ms/step - loss: 0.9387 - accuracy: 0.7597\n",
            "Epoch 180/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 0.9279 - accuracy: 0.7607\n",
            "Epoch 181/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 0.9432 - accuracy: 0.7592\n",
            "Epoch 182/200\n",
            "164/164 [==============================] - 25s 153ms/step - loss: 0.9547 - accuracy: 0.7495\n",
            "Epoch 183/200\n",
            "164/164 [==============================] - 21s 130ms/step - loss: 0.9454 - accuracy: 0.7546\n",
            "Epoch 184/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 0.9230 - accuracy: 0.7605\n",
            "Epoch 185/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 0.8858 - accuracy: 0.7726\n",
            "Epoch 186/200\n",
            "164/164 [==============================] - 20s 120ms/step - loss: 0.8701 - accuracy: 0.7782\n",
            "Epoch 187/200\n",
            "164/164 [==============================] - 20s 121ms/step - loss: 0.8802 - accuracy: 0.7749\n",
            "Epoch 188/200\n",
            "164/164 [==============================] - 20s 120ms/step - loss: 0.8748 - accuracy: 0.7751\n",
            "Epoch 189/200\n",
            "164/164 [==============================] - 24s 149ms/step - loss: 0.8965 - accuracy: 0.7685\n",
            "Epoch 190/200\n",
            "164/164 [==============================] - 20s 122ms/step - loss: 0.8674 - accuracy: 0.7721\n",
            "Epoch 191/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 0.8138 - accuracy: 0.7890\n",
            "Epoch 192/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 0.7749 - accuracy: 0.8055\n",
            "Epoch 193/200\n",
            "164/164 [==============================] - 20s 125ms/step - loss: 0.7547 - accuracy: 0.8116\n",
            "Epoch 194/200\n",
            "164/164 [==============================] - 21s 125ms/step - loss: 0.7481 - accuracy: 0.8120\n",
            "Epoch 195/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 0.7641 - accuracy: 0.8058\n",
            "Epoch 196/200\n",
            "164/164 [==============================] - 20s 124ms/step - loss: 0.7725 - accuracy: 0.8025\n",
            "Epoch 197/200\n",
            "164/164 [==============================] - 21s 128ms/step - loss: 0.7793 - accuracy: 0.7988\n",
            "Epoch 198/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 0.7580 - accuracy: 0.8065\n",
            "Epoch 199/200\n",
            "164/164 [==============================] - 21s 126ms/step - loss: 0.7517 - accuracy: 0.8087\n",
            "Epoch 200/200\n",
            "164/164 [==============================] - 21s 127ms/step - loss: 0.7672 - accuracy: 0.8033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fec0099bf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18NNTPZSvpS"
      },
      "source": [
        "##Using the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn4yVh1EDB3W",
        "outputId": "b7a495ca-4dca-460c-c9cf-03254b3406f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#generating a comment using the model\n",
        "def generate(mod, tokenizer, seq_length, seed_comment, n_words):\n",
        "\tresult = list()\n",
        "\tin_comment = seed_comment\n",
        "\tfor i in range(0, n_words):\n",
        "\t\t#text to integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_comment])[0]\n",
        "\t\t#truncating to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t#predicting probabilities\n",
        "\t\t#prob = mod.predict_classes(encoded, verbose=0)\n",
        "\t\tprob = np.argmax(mod.predict(x), axis=-1)\n",
        "\t\t#word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == prob.any:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t#adding to input\n",
        "\t\tin_comment += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "#calculating the sequence length\n",
        "seq_length = len(stored_sequences[0].split()) - 1\n",
        "\n",
        "#selecting a seed comment\n",
        "seed_comment = stored_sequences[rand.randint(0,len(stored_sequences))]\n",
        " \n",
        "#generating a new comment\n",
        "generated = generate(mod, tokenizer, seq_length, seed_comment, 14)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}